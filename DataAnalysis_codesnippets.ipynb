{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA ANLYSIS\n",
    "\n",
    "# Welch's t test in Python\n",
    "    import numpy\n",
    "    import scipy.stats\n",
    "    import pandas\n",
    "\n",
    "    def compare_averages(filename):\n",
    "        baseball = pandas.read_csv('../Downloads/baseball_stats.csv')\n",
    "        baseball_L = baseball[baseball('handness')] == ['L']\n",
    "        baseball_R = baseball[baseball('handness')] == ['R']\n",
    "        \n",
    "        tuple = scipy.stats.ttest_ind(baseball_L['avg'], baseball_R['avg'], equal_var = False)\n",
    "    return tuple \n",
    "\n",
    "# Shapiro-Wwilk test: Non-normal data\n",
    "    w,p = scipy.stats.shapiro(data)\n",
    "    \n",
    "# Mann-Whitney U test: Non-parametric test\n",
    "    u,p = scipy.stats.mannwhitneyu(x,y)\n",
    "    \n",
    "# LINEAR REGRESSION WITH GRADIENT DESCENT\n",
    "    import numpy\n",
    "    import pandas\n",
    "\n",
    "def compute_cost(features, values, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost of a list of parameters, theta, given a list of features \n",
    "    (input data points) and values (output data points).\n",
    "    \"\"\"\n",
    "    m = len(values)\n",
    "    sum_of_square_errors = numpy.square(numpy.dot(features, theta) - values).sum()\n",
    "    cost = sum_of_square_errors / (2*m)\n",
    "\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(features, values, theta, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent given a data set with an arbitrary number of features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Write code here that performs num_iterations updates to the elements of theta.\n",
    "    # times. Every time you compute the cost for a given list of thetas, append it \n",
    "    # to cost_history.\n",
    "    # See the Instructor notes for hints. \n",
    "    \n",
    "    cost_history = []\n",
    "\n",
    "    ###########################\n",
    "    ### YOUR CODE GOES HERE ###\n",
    "    ###########################\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        predicted_values = numpy.dot(features, theta)\n",
    "        theta = theta - alpha/m * numpy.dot((predicted_values - values), features)\n",
    "        cost = compute.cost(features, values, theta)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "    return theta, pandas.Series(cost_history) \n",
    "\n",
    "    if _name_=='_main_':\n",
    "    # isolate features and values\n",
    "        features = data[['height','weight']]\n",
    "        values = data[['HR']]\n",
    "        m = len(values)\n",
    "    # normalize features\n",
    "        features, mu, sigma = normalize_features(features)\n",
    "        \n",
    "\n",
    "def compute_r_squared(data, predictions):\n",
    "    # Write a function that, given two input numpy arrays, 'data', and 'predictions,'\n",
    "    # returns the coefficient of determination, R^2, for the model that produced \n",
    "    # predictions.\n",
    "    # \n",
    "    # Numpy has a couple of functions -- np.mean() and np.sum() --\n",
    "    # that you might find useful, but you don't have to use them.\n",
    "\n",
    "    # YOUR CODE GOES HERE\n",
    "\n",
    "\n",
    "    numerator = ((data - predictions)**2).sum()\n",
    "    denominator = ((np.mean(data) - data)**2).sum()\n",
    "    r_squared = 1 - numerator/denominator\n",
    "    \n",
    "    return r_squared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
